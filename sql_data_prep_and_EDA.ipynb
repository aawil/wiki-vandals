{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook prepares the previously retrieved Quarry data for analysis, along with some basic exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import geocoder\n",
    "from p5utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Quarry](https://meta.wikimedia.org/wiki/Research:Quarry) is a web interface for requesting data from Wiki Replicas, a set of databases which replicate, in real time, the actual production MariaSQL dbs that all Wikimedia projects run on. All queries made on Quarry can be publicly viewed: [here's mine](https://quarry.wmflabs.org/query/36617), which generated the .tsv file, also reproduced here:\n",
    "\n",
    "```sql\n",
    "use enwiki_p;\n",
    "\n",
    "SELECT rc_timestamp, rc_actor, rc_title, rc_this_oldid, rc_old_len, rc_new_len, actor_name\n",
    "\tFROM recentchanges\n",
    "\t\tLEFT JOIN actor ON rc_actor = actor_id\n",
    "\t\t\tWHERE rc_type = 0\n",
    "\t\t\tAND rc_namespace = 0\n",
    "\t\t\tAND actor_user IS null\n",
    "LIMIT 1000000;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = pd.read_csv('anon_changes_jun04.tsv', sep='\\t')\n",
    "changes['len_diff'] = changes['rc_new_len'] - changes['rc_old_len']\n",
    "changes = changes.query(\"rc_actor != 0.0\")\n",
    "changes = changes.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The damage_chunks data comes from running `get_damage_probs.py` in 1,000-chunk increments simultaneously in four separate terminals (aka \"manual async\"). Each chunk contains the ORES damage probability score for 50 or 51 revisions. Gathering this data for approximately 730,000 revisions took several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_chunks = []\n",
    "for i in range(14588):\n",
    "    damage_chunks.append(pd.read_csv(f'./damage_chunks/damage_chunk_{i}.csv',\n",
    "                             usecols=[1], header=None, squeeze=True))\n",
    "damage_prob = pd.concat(damage_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_prob.to_csv('damage_prob.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes['damage_prob'] = damage_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the damage probabilities are NaN. These are all revisions that were completely deleted by a Wikipedia administrator according to the [revision deletion policy](https://en.wikipedia.org/wiki/Wikipedia:Revision_deletion), where revisions that consist of copyright violations, libel, grossly offensive content, etc. are fully deleted from the page history (rather than simply reverted, like less severe vandalism). I rescored these as 1.0 (i.e., a 100% chance of being damaging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes['damage_prob'] = changes['damage_prob'].fillna(value=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ips = changes['actor_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geoinfo(ip_address, reader):\n",
    "    r = reader.city(ip_address)\n",
    "    return [r.location.latitude,\n",
    "            r.location.longitude,\n",
    "            r.city.name,\n",
    "            r.subdivisions.most_specific.name,\n",
    "            r.country.name,\n",
    "            r.country.iso_code,\n",
    "            r.postal.code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After investigating several IP geocoding services (all of which charged quite a bit of money for coding more than a few thousand IPs, much less several hundred thousand), I found the [GeoLite2 database](https://dev.maxmind.com/geoip/geoip2/geolite2/) offered for free by MaxMind in order to promote their more accurate GeoIP2 data. GeoLite2 is less accurate (the lat/long coordinates are centered in their respective postcodes/cities), but perfectly good for what I needed, free, and (importantly) very fast, since it didn't need to be accessed through a web API.\n",
    "\n",
    "The geoip2 package they offered made coding almost too easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = geoip2.database.Reader('./GeoLite2-City_20190604/GeoLite2-City.mmdb')\n",
    "locs = []\n",
    "for ip in unique_ips:\n",
    "    locs.append(get_geoinfo(ip, reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_locations = pd.concat([pd.Series(unique_ips), (pd.DataFrame(locs))], axis=1)\n",
    "ip_locations.columns = [\n",
    "    'actor_name', 'lat', 'lng', 'city', 'state', 'country', 'country_iso', 'postcode'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = pd.merge(changes, ip_locations, on='actor_name').drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed some NaN values in my article title column, which was unexpected. The cause: several revisions in my dataset were to the [Wikipedia article on the concept of NaN](https://en.wikipedia.org/wiki/NaN), which the Pandas data import coded as literal NaN values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes['rc_title'] = changes['rc_title'].fillna('NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More NA resolving - the GeoLite2 database whiffed on some IPs, so I used the geocoder package to clean up the stragglers. Geocoder uses IPInfo under the hood, which has a 1,000/day rate limit for free access - more than enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ip in changes[changes['lat'].isna()]['actor_name']:\n",
    "    g = geocoder.ip(ip)\n",
    "    changes.loc[changes['actor_name']==ip, 'lat'] = g.lat\n",
    "    changes.loc[changes['actor_name']==ip, 'lng'] = g.lng\n",
    "    changes.loc[changes['actor_name']==ip, 'city'] = g.city\n",
    "    changes.loc[changes['actor_name']==ip, 'state'] = g.state\n",
    "    changes.loc[changes['actor_name']==ip, 'country'] = 'United States'\n",
    "    changes.loc[changes['actor_name']==ip, 'country_iso'] = g.country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a combined lat/long column (for intended use in Google Earth Engine, but I ended up not going that route) and save our dataset to .csv for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes['latlng'] = changes.apply(lambda x: (x['lat'], x['lng']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes.to_csv('changes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out a US-only version to play around with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_us = changes[changes['country_iso']=='US']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what people in my city and ZIP code are editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Edwardsville,_Illinois', 'My_Bloody_Valentine_(band)',\n",
       "       'Christine_(name)', 'Probation', 'Shepley,_Rutan_and_Coolidge'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(changes[changes['city']=='Chicago']['rc_title'].unique(), size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tom_Cat                                          16\n",
       "Tom_and_Jerry:_Robin_Hood_and_His_Merry_Mouse     9\n",
       "Baby_Puss                                         7\n",
       "Varsity_(band)                                    6\n",
       "Song_Review:_A_Greatest_Hits_Collection           5\n",
       "Name: rc_title, dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changes[changes['postcode']=='60647']['rc_title'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh, weird. What about the top edited articles in the entire dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List_of_Produce_X_101_contestants    383\n",
       "2019_Indian_general_election         377\n",
       "Kenzō_Shirai                         356\n",
       "2019–20_UEFA_Champions_League        343\n",
       "Aladdin_(2019_film)                  325\n",
       "Name: rc_title, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changes['rc_title'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the deleted/added text fields look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted:\n",
      " []\n",
      "Added:\n",
      " ['', '===Claims===', \"Journalist, scholar and author [[Toby Lester]] summarizes the  claims of Hagarism's at odds with Islamic doctrine (followed by quotes of the authors in parenthesis):\", '*\"suggestions that the text of the Quran came into being later than is now believed\" (\"There is no hard evidence for the existence of the Koran in any form before the last decade of the seventh century\"); ', '*\"that [[Mecca]] was not the initial Islamic sanctuary\" (\"[the evidence] points unambiguously to a sanctuary in north-west Arabia ... Mecca was secondary\"); ', '*\"that the Arab conquests preceded the institutionalization of Islam (\"the Jewish messianic fantasy was enacted in the form of an Arab conquest of the Holy Land\"); ', '*\"that the idea of the [[Hegira|hijra]], or the migration of Muhammad and his followers from Mecca to [[Medina]] in 622, may have evolved long after Muhammad died\" (\"No seventh-century source identifies the Arab era as that of the hijra\"); and ', '*\"that the term \"Muslim\" was not commonly used in early Islam\" (\"There is no good reason to suppose that the bearers of this primitive identity called themselves \\'Muslims\\' [but] sources do ... reveal an earlier designation of the community [which] appears in Greek as \\'Magaritai\\' in a papyrus of 642, and in Syriac as \\'Mahgre\\' or \\'Mahgraye\\' from as early as the 640s\")\".<ref name=\"what-atlantic-1999\">{{cite journal |journal=Atlantic |last1=LESTER |first1=TOBY |date= January 1999  |title=What Is the Koran? | url=https://www.theatlantic.com/magazine/archive/1999/01/what-is-the-koran/304024/ |accessdate=8 April 2019}}</ref>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaron/metis/project-05/p5utils.py:7: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 7 of the file /Users/aaron/metis/project-05/p5utils.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(response['compare']['*'], features='lxml')\n"
     ]
    }
   ],
   "source": [
    "get_changed_text(900103915)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most vandalous countries are likely just variance due to very small N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country\n",
       "French Guiana                        0.953278\n",
       "Bonaire, Sint Eustatius, and Saba    0.739908\n",
       "Gabon                                0.685797\n",
       "Anguilla                             0.681811\n",
       "Comoros                              0.671198\n",
       "Name: damage_prob, dtype: float64"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changes.groupby('country')['damage_prob'].mean().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "Wyoming          0.540221\n",
       "Iowa             0.493879\n",
       "New Hampshire    0.487237\n",
       "Delaware         0.475978\n",
       "Mississippi      0.473873\n",
       "Name: damage_prob, dtype: float64"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changes_us.groupby('state')['damage_prob'].mean().sort_values(ascending=False).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
